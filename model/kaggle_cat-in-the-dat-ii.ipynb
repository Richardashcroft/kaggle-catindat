{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"kaggle_cat-in-the-dat-ii.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1J3t6vxc_GP_q5-1cPQQsWbhOFkCjLcey","authorship_tag":"ABX9TyP33wk7IiLp2sfjPaGkm9en"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"poFlNYsI5VCl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"367e099a-be5a-4e0f-ed98-6d2ae05ac300","executionInfo":{"status":"ok","timestamp":1585463861199,"user_tz":-540,"elapsed":1055,"user":{"displayName":"迅行","photoUrl":"","userId":"02533846765546745964"}}},"source":["%cd '/content/drive/My Drive/Colab Notebooks/kaggle/cat-in-the-dat-ii'\n","#'/content/drive/My Drive/Colab Notebooks/kaggle/cat-in-the-dat-ii'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/kaggle/cat-in-the-dat-ii\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ucJfzQCI_ybO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"75149725-c042-45de-bed3-309e27812ba4","executionInfo":{"status":"ok","timestamp":1585463791175,"user_tz":-540,"elapsed":136841,"user":{"displayName":"迅行","photoUrl":"","userId":"02533846765546745964"}}},"source":["!pip install tensorflow==2.1.0"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==2.1.0\n","  Downloading tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8 MB)\n","\u001b[K     |████████████████████████████████| 421.8 MB 25 kB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.2.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.10.0)\n","Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n","  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 45.6 MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.27.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.8.1)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.34.2)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.9.0)\n","Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (2.1.1)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.18.2)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.0.8)\n","Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.4.1)\n","Collecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow==2.1.0) (46.0.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.21.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.7.2)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.2.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2019.11.28)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n","Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n","Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=b2a2ed7526ae1df08df8fbfdb71173c66568751ef0e055617c20066f7e7af129\n","  Stored in directory: /root/.cache/pip/wheels/19/a7/b9/0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\n","Successfully built gast\n","Installing collected packages: tensorflow-estimator, gast, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.2.0rc0\n","    Uninstalling tensorflow-estimator-2.2.0rc0:\n","      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.2.0rc1\n","    Uninstalling tensorflow-2.2.0rc1:\n","      Successfully uninstalled tensorflow-2.2.0rc1\n","Successfully installed gast-0.2.2 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gast","tensorflow"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"0-BlO6Cr_4oS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"outputId":"04ebe8cb-4f72-4a7d-b798-af44b58604d2","executionInfo":{"status":"ok","timestamp":1585463846111,"user_tz":-540,"elapsed":4549,"user":{"displayName":"迅行","photoUrl":"","userId":"02533846765546745964"}}},"source":["!pip install keras==2.3.1"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Collecting keras==2.3.1\n","  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n","\u001b[?25l\r\u001b[K     |▉                               | 10 kB 39.3 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 71 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 81 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 215 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 266 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 276 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 286 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 317 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 327 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 337 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 368 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 377 kB 9.8 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.18.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (2.10.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.4.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.12.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (3.13)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.0.8)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.3.1) (1.1.0)\n","Installing collected packages: keras\n","  Attempting uninstall: keras\n","    Found existing installation: Keras 2.2.5\n","    Uninstalling Keras-2.2.5:\n","      Successfully uninstalled Keras-2.2.5\n","Successfully installed keras-2.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v9eWTUI47E17","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":646},"outputId":"2e612423-29cf-45eb-feae-c4108a55c9f2","executionInfo":{"status":"ok","timestamp":1585462641822,"user_tz":-540,"elapsed":15845,"user":{"displayName":"迅行","photoUrl":"","userId":"02533846765546745964"}}},"source":["#!pip install kaggle==1.5.6\n","#!pip install --upgrade kaggle\n","\n","# for colab:\n","!pip uninstall -y kaggle\n","!pip install --upgrade pip\n","!pip install kaggle==1.5.6\n","!kaggle -v"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Uninstalling kaggle-1.5.6:\n","  Successfully uninstalled kaggle-1.5.6\n","Collecting pip\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/0c/d01aa759fdc501a58f431eb594a17495f15b88da142ce14b5845662c13f3/pip-20.0.2-py2.py3-none-any.whl (1.4MB)\n","\u001b[K     |████████████████████████████████| 1.4MB 7.7MB/s \n","\u001b[?25hInstalling collected packages: pip\n","  Found existing installation: pip 19.3.1\n","    Uninstalling pip-19.3.1:\n","      Successfully uninstalled pip-19.3.1\n","Successfully installed pip-20.0.2\n","Collecting kaggle==1.5.6\n","  Downloading kaggle-1.5.6.tar.gz (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 3.0 MB/s \n","\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.24.3)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (1.12.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2019.11.28)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (2.21.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (4.38.0)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle==1.5.6) (4.0.0)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle==1.5.6) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle==1.5.6) (3.0.4)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle==1.5.6) (1.3)\n","Building wheels for collected packages: kaggle\n","  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kaggle: filename=kaggle-1.5.6-py3-none-any.whl size=72859 sha256=8e3aadff43c664f76235af338bf3738c0568a337a3583c41c7a3d77a46854685\n","  Stored in directory: /root/.cache/pip/wheels/01/3e/ff/77407ebac3ef71a79b9166a8382aecf88415a0bcbe3c095a01\n","Successfully built kaggle\n","Installing collected packages: kaggle\n","Successfully installed kaggle-1.5.6\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["kaggle"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["Kaggle API 1.5.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vhlfmo0O6Lq5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ead2e50d-478c-4a65-8ed2-a9a9c956bb0e","executionInfo":{"status":"ok","timestamp":1585462235134,"user_tz":-540,"elapsed":11120,"user":{"displayName":"迅行","photoUrl":"","userId":"02533846765546745964"}}},"source":["!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!ls ~/.kaggle\n","!chmod 600 /root/.kaggle/kaggle.json"],"execution_count":5,"outputs":[{"output_type":"stream","text":["kaggle.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jiaaSRCD5zym","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"bb073e73-2c23-42b8-c477-6d211a6fe231","executionInfo":{"status":"ok","timestamp":1585462662221,"user_tz":-540,"elapsed":2727,"user":{"displayName":"迅行","photoUrl":"","userId":"02533846765546745964"}}},"source":["#import kaggle\n","!kaggle --version"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Kaggle API 1.5.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MufwLCbf6Edp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"outputId":"403b7329-e16a-4157-e5eb-05cd9c14943a","executionInfo":{"status":"ok","timestamp":1585462850386,"user_tz":-540,"elapsed":4584,"user":{"displayName":"迅行","photoUrl":"","userId":"02533846765546745964"}}},"source":["!kaggle competitions download -c cat-in-the-dat-ii"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading cat-in-the-dat-ii.zip to /content/drive/My Drive/Colab Notebooks/kaggle\n"," 80% 33.0M/41.3M [00:00<00:00, 27.5MB/s]\n","100% 41.3M/41.3M [00:00<00:00, 46.3MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o2xAQXNO-Or5","colab_type":"code","colab":{}},"source":["#from google.colab import drive\n","#drive.mount('/content/drive/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KwRFFjxO64f6","colab_type":"code","colab":{}},"source":["#!unzip  source.zip -d destination.zip\n","\n","import zipfile\n","zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Colab Notebooks/kaggle/cat-in-the-dat-ii/cat-in-the-dat-ii.zip\", 'r')\n","zip_ref.extractall(\"/tmp\") # should delete '/tmp' ?\n","zip_ref.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C_BT8jci-crn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"388706bf-041e-4586-fdc0-bc09d9501d34","executionInfo":{"status":"ok","timestamp":1585463534627,"user_tz":-540,"elapsed":4116,"user":{"displayName":"迅行","photoUrl":"","userId":"02533846765546745964"}}},"source":["!unzip \"/content/drive/My Drive/Colab Notebooks/kaggle/cat-in-the-dat-ii/cat-in-the-dat-ii.zip\""],"execution_count":7,"outputs":[{"output_type":"stream","text":["Archive:  /content/drive/My Drive/Colab Notebooks/kaggle/cat-in-the-dat-ii/cat-in-the-dat-ii.zip\n","  inflating: sample_submission.csv   \n","  inflating: test.csv                \n","  inflating: train.csv               \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mPLKCJY0_ZNH","colab_type":"code","colab":{}},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import os\n","import gc\n","import joblib\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn import metrics, preprocessing\n","from tensorflow.keras import layers\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras import callbacks\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import utils"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Ejo7qGv_sm7","colab_type":"code","colab":{}},"source":["# define metric of AUC\n","def auc(y_true, y_pred):\n","    def fallback_auc(y_true, y_pred):\n","        try:\n","            return metrics.roc_auc_score(y_true, y_pred)\n","        except:\n","            return 0.5\n","    return tf.py_function(fallback_auc, (y_true, y_pred), tf.double)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CRc6YO2RA52u","colab_type":"code","colab":{}},"source":["train = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/kaggle/cat-in-the-dat-ii/train.csv\")\n","test = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/kaggle/cat-in-the-dat-ii/test.csv\")\n","sample = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/kaggle/cat-in-the-dat-ii/sample_submission.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bu72n7Y9A8ab","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":249},"outputId":"71eb597a-e0f2-47c6-c9d2-d9d923d1c09f","executionInfo":{"status":"ok","timestamp":1585464024260,"user_tz":-540,"elapsed":1091,"user":{"displayName":"迅行","photoUrl":"","userId":"02533846765546745964"}}},"source":["train.head()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>bin_0</th>\n","      <th>bin_1</th>\n","      <th>bin_2</th>\n","      <th>bin_3</th>\n","      <th>bin_4</th>\n","      <th>nom_0</th>\n","      <th>nom_1</th>\n","      <th>nom_2</th>\n","      <th>nom_3</th>\n","      <th>nom_4</th>\n","      <th>nom_5</th>\n","      <th>nom_6</th>\n","      <th>nom_7</th>\n","      <th>nom_8</th>\n","      <th>nom_9</th>\n","      <th>ord_0</th>\n","      <th>ord_1</th>\n","      <th>ord_2</th>\n","      <th>ord_3</th>\n","      <th>ord_4</th>\n","      <th>ord_5</th>\n","      <th>day</th>\n","      <th>month</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>Red</td>\n","      <td>Trapezoid</td>\n","      <td>Hamster</td>\n","      <td>Russia</td>\n","      <td>Bassoon</td>\n","      <td>de4c57ee2</td>\n","      <td>a64bc7ddf</td>\n","      <td>598080a91</td>\n","      <td>0256c7a4b</td>\n","      <td>02e7c8990</td>\n","      <td>3.0</td>\n","      <td>Contributor</td>\n","      <td>Hot</td>\n","      <td>c</td>\n","      <td>U</td>\n","      <td>Pw</td>\n","      <td>6.0</td>\n","      <td>3.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>F</td>\n","      <td>Y</td>\n","      <td>Red</td>\n","      <td>Star</td>\n","      <td>Axolotl</td>\n","      <td>NaN</td>\n","      <td>Theremin</td>\n","      <td>2bb3c3e5c</td>\n","      <td>3a3a936e8</td>\n","      <td>1dddb8473</td>\n","      <td>52ead350c</td>\n","      <td>f37df64af</td>\n","      <td>3.0</td>\n","      <td>Grandmaster</td>\n","      <td>Warm</td>\n","      <td>e</td>\n","      <td>X</td>\n","      <td>pE</td>\n","      <td>7.0</td>\n","      <td>7.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>Red</td>\n","      <td>NaN</td>\n","      <td>Hamster</td>\n","      <td>Canada</td>\n","      <td>Bassoon</td>\n","      <td>b574c9841</td>\n","      <td>708248125</td>\n","      <td>5ddc9a726</td>\n","      <td>745b909d1</td>\n","      <td>NaN</td>\n","      <td>3.0</td>\n","      <td>NaN</td>\n","      <td>Freezing</td>\n","      <td>n</td>\n","      <td>P</td>\n","      <td>eN</td>\n","      <td>5.0</td>\n","      <td>9.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>Red</td>\n","      <td>Circle</td>\n","      <td>Hamster</td>\n","      <td>Finland</td>\n","      <td>Theremin</td>\n","      <td>673bdf1f6</td>\n","      <td>23edb8da3</td>\n","      <td>3a33ef960</td>\n","      <td>bdaa56dd1</td>\n","      <td>f9d456e57</td>\n","      <td>1.0</td>\n","      <td>Novice</td>\n","      <td>Lava Hot</td>\n","      <td>a</td>\n","      <td>C</td>\n","      <td>NaN</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>NaN</td>\n","      <td>0.0</td>\n","      <td>T</td>\n","      <td>N</td>\n","      <td>Red</td>\n","      <td>Triangle</td>\n","      <td>Hamster</td>\n","      <td>Costa Rica</td>\n","      <td>NaN</td>\n","      <td>777d1ac2c</td>\n","      <td>3a7975e46</td>\n","      <td>bc9cc2a94</td>\n","      <td>NaN</td>\n","      <td>c5361037c</td>\n","      <td>3.0</td>\n","      <td>Grandmaster</td>\n","      <td>Cold</td>\n","      <td>h</td>\n","      <td>C</td>\n","      <td>OZ</td>\n","      <td>5.0</td>\n","      <td>12.0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  bin_0  bin_1  bin_2 bin_3 bin_4  ... ord_3 ord_4 ord_5  day month target\n","0   0    0.0    0.0    0.0     F     N  ...     c     U    Pw  6.0   3.0      0\n","1   1    1.0    1.0    0.0     F     Y  ...     e     X    pE  7.0   7.0      0\n","2   2    0.0    1.0    0.0     F     N  ...     n     P    eN  5.0   9.0      0\n","3   3    NaN    0.0    0.0     F     N  ...     a     C   NaN  3.0   3.0      0\n","4   4    0.0    NaN    0.0     T     N  ...     h     C    OZ  5.0  12.0      0\n","\n","[5 rows x 25 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"gUwNJP-zBReZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":521},"outputId":"afc53bd5-f7a2-44e9-d99b-95718a63ff41","executionInfo":{"status":"ok","timestamp":1585464043525,"user_tz":-540,"elapsed":1251,"user":{"displayName":"迅行","photoUrl":"","userId":"02533846765546745964"}}},"source":["train.info()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 600000 entries, 0 to 599999\n","Data columns (total 25 columns):\n","id        600000 non-null int64\n","bin_0     582106 non-null float64\n","bin_1     581997 non-null float64\n","bin_2     582070 non-null float64\n","bin_3     581986 non-null object\n","bin_4     581953 non-null object\n","nom_0     581748 non-null object\n","nom_1     581844 non-null object\n","nom_2     581965 non-null object\n","nom_3     581879 non-null object\n","nom_4     581965 non-null object\n","nom_5     582222 non-null object\n","nom_6     581869 non-null object\n","nom_7     581997 non-null object\n","nom_8     582245 non-null object\n","nom_9     581927 non-null object\n","ord_0     581712 non-null float64\n","ord_1     581959 non-null object\n","ord_2     581925 non-null object\n","ord_3     582084 non-null object\n","ord_4     582070 non-null object\n","ord_5     582287 non-null object\n","day       582048 non-null float64\n","month     582012 non-null float64\n","target    600000 non-null int64\n","dtypes: float64(6), int64(2), object(17)\n","memory usage: 114.4+ MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J3-Fba1hA3NI","colab_type":"code","colab":{}},"source":["def create_model(data, catcols):    \n","    inputs = []\n","    outputs = []\n","    for c in catcols:\n","        num_unique_values = int(data[c].nunique())\n","        embed_dim = int(min(np.ceil((num_unique_values)/2), 50))\n","        inp = layers.Input(shape=(1,))\n","        out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n","        out = layers.SpatialDropout1D(0.3)(out)\n","        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n","        inputs.append(inp)\n","        outputs.append(out)\n","    \n","    x = layers.Concatenate()(outputs)\n","    x = layers.BatchNormalization()(x)\n","    \n","    x = layers.Dense(300, activation=\"relu\")(x)\n","    x = layers.Dropout(0.3)(x)\n","    x = layers.BatchNormalization()(x)\n","    \n","    x = layers.Dense(300, activation=\"relu\")(x)\n","    x = layers.Dropout(0.3)(x)\n","    x = layers.BatchNormalization()(x)\n","    \n","    y = layers.Dense(2, activation=\"softmax\")(x)\n","\n","    model = Model(inputs=inputs, outputs=y)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MiHucJ6yBWG-","colab_type":"code","colab":{}},"source":["test[\"target\"] = -1\n","data = pd.concat([train, test]).reset_index(drop=True)\n","\n","features = [x for x in train.columns if x not in [\"id\", \"target\"]]\n","\n","for feat in features:\n","    lbl_enc = preprocessing.LabelEncoder()\n","    data[feat] = lbl_enc.fit_transform(data[feat].fillna(\"-1\").astype(str).values)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S-U0XMomBpCv","colab_type":"code","colab":{}},"source":["train = data[data.target != -1].reset_index(drop=True)\n","test = data[data.target == -1].reset_index(drop=True)\n","test_data = [test.loc[:, features].values[:, k] for k in range(test.loc[:, features].values.shape[1])]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tU0rkKO6B4fH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"5995e1fd-ca8e-4bfd-d214-bd6017a4f1cf","executionInfo":{"status":"ok","timestamp":1585472222738,"user_tz":-540,"elapsed":7652521,"user":{"displayName":"迅行","photoUrl":"","userId":"02533846765546745964"}}},"source":["oof_preds = np.zeros((len(train)))\n","test_preds = np.zeros((len(test)))\n","\n","skf = StratifiedKFold(n_splits=50)\n","for train_index, test_index in skf.split(train, train.target.values):\n","    X_train, X_test = train.iloc[train_index, :], train.iloc[test_index, :]\n","    X_train = X_train.reset_index(drop=True)\n","    X_test = X_test.reset_index(drop=True)\n","    y_train, y_test = X_train.target.values, X_test.target.values\n","    model = create_model(data, features)\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\n","    X_train = [X_train.loc[:, features].values[:, k] for k in range(X_train.loc[:, features].values.shape[1])]\n","    X_test = [X_test.loc[:, features].values[:, k] for k in range(X_test.loc[:, features].values.shape[1])]\n","    \n","    es = callbacks.EarlyStopping(monitor='val_auc', min_delta=0.001, patience=5,\n","                                 verbose=1, mode='max', baseline=None, restore_best_weights=True)\n","\n","    rlr = callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5,\n","                                      patience=3, min_lr=1e-6, mode='max', verbose=1)\n","    \n","    model.fit(X_train,\n","              utils.to_categorical(y_train),\n","              validation_data=(X_test, utils.to_categorical(y_test)),\n","              verbose=1,\n","              batch_size=1024,\n","              callbacks=[es, rlr],\n","              epochs=50\n","             )\n","    valid_fold_preds = model.predict(X_test)[:, 1]\n","    test_fold_preds = model.predict(test_data)[:, 1]\n","    oof_preds[test_index] = valid_fold_preds.ravel()\n","    test_preds += test_fold_preds.ravel()\n","    print(metrics.roc_auc_score(y_test, valid_fold_preds))\n","    K.clear_session()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 19s 32us/sample - loss: 0.4707 - auc: 0.6995 - val_loss: 0.4095 - val_auc: 0.7733\n","Epoch 2/50\n","588000/588000 [==============================] - 13s 22us/sample - loss: 0.4085 - auc: 0.7698 - val_loss: 0.4017 - val_auc: 0.7800\n","Epoch 3/50\n","588000/588000 [==============================] - 13s 21us/sample - loss: 0.4030 - auc: 0.7789 - val_loss: 0.4011 - val_auc: 0.7809\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4006 - auc: 0.7826 - val_loss: 0.4027 - val_auc: 0.7799\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3986 - auc: 0.7854 - val_loss: 0.3998 - val_auc: 0.7812\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3969 - auc: 0.7879 - val_loss: 0.4032 - val_auc: 0.7788\n","Epoch 7/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3943 - auc: 0.7918 - val_loss: 0.4048 - val_auc: 0.7782\n","Epoch 8/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3902 - auc: 0.7980\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3902 - auc: 0.7981 - val_loss: 0.4081 - val_auc: 0.7729\n","Epoch 9/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3806 - auc: 0.8110 - val_loss: 0.4158 - val_auc: 0.7651\n","Epoch 10/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3760 - auc: 0.8173Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3761 - auc: 0.8172 - val_loss: 0.4188 - val_auc: 0.7631\n","Epoch 00010: early stopping\n","0.7810593402692887\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4712 - auc: 0.6972 - val_loss: 0.4025 - val_auc: 0.7880\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4085 - auc: 0.7695 - val_loss: 0.3931 - val_auc: 0.7946\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4035 - auc: 0.7783 - val_loss: 0.3939 - val_auc: 0.7954\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4009 - auc: 0.7820 - val_loss: 0.3940 - val_auc: 0.7939\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3990 - auc: 0.7847 - val_loss: 0.3941 - val_auc: 0.7945\n","Epoch 6/50\n","585728/588000 [============================>.] - ETA: 0s - loss: 0.3963 - auc: 0.7885\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3963 - auc: 0.7886 - val_loss: 0.3964 - val_auc: 0.7948\n","Epoch 7/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3905 - auc: 0.7967Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3905 - auc: 0.7967 - val_loss: 0.3969 - val_auc: 0.7915\n","Epoch 00007: early stopping\n","0.794864120407437\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 26us/sample - loss: 0.4692 - auc: 0.7010 - val_loss: 0.4067 - val_auc: 0.7864\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4082 - auc: 0.7702 - val_loss: 0.3957 - val_auc: 0.7930\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4032 - auc: 0.7784 - val_loss: 0.3954 - val_auc: 0.7935\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4009 - auc: 0.7815 - val_loss: 0.3966 - val_auc: 0.7926\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3989 - auc: 0.7849 - val_loss: 0.3961 - val_auc: 0.7927\n","Epoch 6/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3963 - auc: 0.7888\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3963 - auc: 0.7888 - val_loss: 0.3997 - val_auc: 0.7884\n","Epoch 7/50\n","585728/588000 [============================>.] - ETA: 0s - loss: 0.3901 - auc: 0.7977Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3901 - auc: 0.7976 - val_loss: 0.3991 - val_auc: 0.7870\n","Epoch 00007: early stopping\n","0.7918804368406704\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4692 - auc: 0.6992 - val_loss: 0.4128 - val_auc: 0.7751\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4087 - auc: 0.7694 - val_loss: 0.4033 - val_auc: 0.7781\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4034 - auc: 0.7780 - val_loss: 0.4050 - val_auc: 0.7771\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4003 - auc: 0.7829 - val_loss: 0.4038 - val_auc: 0.7774\n","Epoch 5/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3984 - auc: 0.7858\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3984 - auc: 0.7858 - val_loss: 0.4040 - val_auc: 0.7763\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3935 - auc: 0.7922 - val_loss: 0.4066 - val_auc: 0.7754\n","Epoch 7/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3901 - auc: 0.7975Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3901 - auc: 0.7976 - val_loss: 0.4099 - val_auc: 0.7746\n","Epoch 00007: early stopping\n","0.7768637192659823\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4673 - auc: 0.7005 - val_loss: 0.4116 - val_auc: 0.7763\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4079 - auc: 0.7708 - val_loss: 0.4025 - val_auc: 0.7790\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4031 - auc: 0.7783 - val_loss: 0.4023 - val_auc: 0.7793\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4007 - auc: 0.7823 - val_loss: 0.4043 - val_auc: 0.7778\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3985 - auc: 0.7854 - val_loss: 0.4053 - val_auc: 0.7754\n","Epoch 6/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3963 - auc: 0.7885\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3962 - auc: 0.7886 - val_loss: 0.4104 - val_auc: 0.7735\n","Epoch 7/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3898 - auc: 0.7982Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3898 - auc: 0.7981 - val_loss: 0.4089 - val_auc: 0.7685\n","Epoch 00007: early stopping\n","0.7788279566927906\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4693 - auc: 0.7002 - val_loss: 0.4063 - val_auc: 0.7825\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4084 - auc: 0.7702 - val_loss: 0.3981 - val_auc: 0.7848\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4030 - auc: 0.7787 - val_loss: 0.3988 - val_auc: 0.7848\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4009 - auc: 0.7818 - val_loss: 0.3983 - val_auc: 0.7848\n","Epoch 5/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3991 - auc: 0.7846\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3992 - auc: 0.7846 - val_loss: 0.4014 - val_auc: 0.7821\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3932 - auc: 0.7928 - val_loss: 0.4012 - val_auc: 0.7812\n","Epoch 7/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3909 - auc: 0.7962Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3909 - auc: 0.7961 - val_loss: 0.4028 - val_auc: 0.7797\n","Epoch 00007: early stopping\n","0.7854555091766814\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 15s 26us/sample - loss: 0.4681 - auc: 0.7023 - val_loss: 0.4096 - val_auc: 0.7780\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4085 - auc: 0.7699 - val_loss: 0.4007 - val_auc: 0.7809\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4035 - auc: 0.7780 - val_loss: 0.4023 - val_auc: 0.7797\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4005 - auc: 0.7825 - val_loss: 0.4026 - val_auc: 0.7792\n","Epoch 5/50\n","585728/588000 [============================>.] - ETA: 0s - loss: 0.3990 - auc: 0.7848\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3990 - auc: 0.7848 - val_loss: 0.4016 - val_auc: 0.7805\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3933 - auc: 0.7926 - val_loss: 0.4039 - val_auc: 0.7772\n","Epoch 7/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3910 - auc: 0.7961Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3910 - auc: 0.7962 - val_loss: 0.4069 - val_auc: 0.7736\n","Epoch 00007: early stopping\n","0.7797724512773809\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 26us/sample - loss: 0.4703 - auc: 0.6992 - val_loss: 0.4110 - val_auc: 0.7804\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4087 - auc: 0.7694 - val_loss: 0.3986 - val_auc: 0.7847\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4034 - auc: 0.7781 - val_loss: 0.3979 - val_auc: 0.7872\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4008 - auc: 0.7820 - val_loss: 0.3978 - val_auc: 0.7877\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3989 - auc: 0.7850 - val_loss: 0.3987 - val_auc: 0.7867\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3961 - auc: 0.7890 - val_loss: 0.4017 - val_auc: 0.7835\n","Epoch 7/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3932 - auc: 0.7935\n","Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3931 - auc: 0.7934 - val_loss: 0.4020 - val_auc: 0.7835\n","Epoch 8/50\n","585728/588000 [============================>.] - ETA: 0s - loss: 0.3850 - auc: 0.8046Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3850 - auc: 0.8045 - val_loss: 0.4073 - val_auc: 0.7790\n","Epoch 00008: early stopping\n","0.7864682224576771\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4690 - auc: 0.7002 - val_loss: 0.4120 - val_auc: 0.7855\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4087 - auc: 0.7693 - val_loss: 0.3982 - val_auc: 0.7899\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4031 - auc: 0.7781 - val_loss: 0.3964 - val_auc: 0.7912\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4008 - auc: 0.7821 - val_loss: 0.3974 - val_auc: 0.7893\n","Epoch 5/50\n","588000/588000 [==============================] - 11s 19us/sample - loss: 0.3987 - auc: 0.7854 - val_loss: 0.3965 - val_auc: 0.7892\n","Epoch 6/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3962 - auc: 0.7887\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 11s 19us/sample - loss: 0.3963 - auc: 0.7887 - val_loss: 0.4023 - val_auc: 0.7845\n","Epoch 7/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3896 - auc: 0.7983 - val_loss: 0.4055 - val_auc: 0.7837\n","Epoch 8/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3861 - auc: 0.8033Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3861 - auc: 0.8033 - val_loss: 0.4059 - val_auc: 0.7799\n","Epoch 00008: early stopping\n","0.7899750149309707\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4713 - auc: 0.6982 - val_loss: 0.4087 - val_auc: 0.7543\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4086 - auc: 0.7696 - val_loss: 0.3990 - val_auc: 0.7608\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4032 - auc: 0.7784 - val_loss: 0.3993 - val_auc: 0.7597\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4009 - auc: 0.7819 - val_loss: 0.3972 - val_auc: 0.7608\n","Epoch 5/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3986 - auc: 0.7853\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3987 - auc: 0.7851 - val_loss: 0.3989 - val_auc: 0.7592\n","Epoch 6/50\n","588000/588000 [==============================] - 11s 19us/sample - loss: 0.3941 - auc: 0.7918 - val_loss: 0.4004 - val_auc: 0.7577\n","Epoch 7/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3912 - auc: 0.7959Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3912 - auc: 0.7959 - val_loss: 0.4028 - val_auc: 0.7553\n","Epoch 00007: early stopping\n","0.7860532957595678\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4670 - auc: 0.7026 - val_loss: 0.4069 - val_auc: 0.7623\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4084 - auc: 0.7701 - val_loss: 0.3969 - val_auc: 0.7731\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4030 - auc: 0.7787 - val_loss: 0.3965 - val_auc: 0.7761\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4006 - auc: 0.7826 - val_loss: 0.3986 - val_auc: 0.7800\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3984 - auc: 0.7856 - val_loss: 0.4001 - val_auc: 0.7749\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3965 - auc: 0.7885 - val_loss: 0.4006 - val_auc: 0.7741\n","Epoch 7/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3935 - auc: 0.7929\n","Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3935 - auc: 0.7928 - val_loss: 0.4028 - val_auc: 0.7733\n","Epoch 8/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3855 - auc: 0.8043 - val_loss: 0.4053 - val_auc: 0.7723\n","Epoch 9/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3809 - auc: 0.8104Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3809 - auc: 0.8103 - val_loss: 0.4094 - val_auc: 0.7634\n","Epoch 00009: early stopping\n","0.7875385416234927\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 17s 28us/sample - loss: 0.4716 - auc: 0.6968 - val_loss: 0.4105 - val_auc: 0.7593\n","Epoch 2/50\n","588000/588000 [==============================] - 13s 21us/sample - loss: 0.4085 - auc: 0.7698 - val_loss: 0.3995 - val_auc: 0.7607\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4031 - auc: 0.7785 - val_loss: 0.3983 - val_auc: 0.7629\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4003 - auc: 0.7825 - val_loss: 0.3997 - val_auc: 0.7611\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3985 - auc: 0.7852 - val_loss: 0.4001 - val_auc: 0.7619\n","Epoch 6/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3972 - auc: 0.7876\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 13s 21us/sample - loss: 0.3972 - auc: 0.7876 - val_loss: 0.4024 - val_auc: 0.7605\n","Epoch 7/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3906 - auc: 0.7965 - val_loss: 0.4043 - val_auc: 0.7583\n","Epoch 8/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3878 - auc: 0.8008Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3878 - auc: 0.8007 - val_loss: 0.4060 - val_auc: 0.7548\n","Epoch 00008: early stopping\n","0.7876018533209931\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 28us/sample - loss: 0.4685 - auc: 0.6992 - val_loss: 0.4070 - val_auc: 0.7872\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4084 - auc: 0.7699 - val_loss: 0.3914 - val_auc: 0.7929\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4035 - auc: 0.7780 - val_loss: 0.3945 - val_auc: 0.7925\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4006 - auc: 0.7823 - val_loss: 0.3934 - val_auc: 0.7905\n","Epoch 5/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3989 - auc: 0.7848\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3990 - auc: 0.7845 - val_loss: 0.3943 - val_auc: 0.7888\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3933 - auc: 0.7928 - val_loss: 0.3956 - val_auc: 0.7899\n","Epoch 7/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3909 - auc: 0.7963Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3909 - auc: 0.7964 - val_loss: 0.3967 - val_auc: 0.7896\n","Epoch 00007: early stopping\n","0.7955114562676389\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 17s 29us/sample - loss: 0.4697 - auc: 0.7003 - val_loss: 0.4062 - val_auc: 0.7588\n","Epoch 2/50\n","588000/588000 [==============================] - 13s 21us/sample - loss: 0.4084 - auc: 0.7698 - val_loss: 0.3995 - val_auc: 0.7612\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4034 - auc: 0.7782 - val_loss: 0.3970 - val_auc: 0.7636\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4008 - auc: 0.7821 - val_loss: 0.3990 - val_auc: 0.7633\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3991 - auc: 0.7848 - val_loss: 0.3980 - val_auc: 0.7633\n","Epoch 6/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3969 - auc: 0.7877\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3969 - auc: 0.7877 - val_loss: 0.3984 - val_auc: 0.7630\n","Epoch 7/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3910 - auc: 0.7961 - val_loss: 0.4002 - val_auc: 0.7608\n","Epoch 8/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3873 - auc: 0.8015Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3874 - auc: 0.8014 - val_loss: 0.4035 - val_auc: 0.7588\n","Epoch 00008: early stopping\n","0.7859850085934104\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4693 - auc: 0.6971 - val_loss: 0.4177 - val_auc: 0.7490\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4080 - auc: 0.7706 - val_loss: 0.4038 - val_auc: 0.7526\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4033 - auc: 0.7785 - val_loss: 0.4041 - val_auc: 0.7524\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4004 - auc: 0.7830 - val_loss: 0.4049 - val_auc: 0.7532\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3988 - auc: 0.7852 - val_loss: 0.4048 - val_auc: 0.7513\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3964 - auc: 0.7884 - val_loss: 0.4065 - val_auc: 0.7496\n","Epoch 7/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3932 - auc: 0.7935Restoring model weights from the end of the best epoch.\n","\n","Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3932 - auc: 0.7934 - val_loss: 0.4105 - val_auc: 0.7449\n","Epoch 00007: early stopping\n","0.7750713865636052\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4697 - auc: 0.6988 - val_loss: 0.4052 - val_auc: 0.7582\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4083 - auc: 0.7700 - val_loss: 0.3978 - val_auc: 0.7628\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4033 - auc: 0.7782 - val_loss: 0.3962 - val_auc: 0.7644\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4007 - auc: 0.7822 - val_loss: 0.3969 - val_auc: 0.7638\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3988 - auc: 0.7848 - val_loss: 0.3987 - val_auc: 0.7630\n","Epoch 6/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3968 - auc: 0.7880\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3968 - auc: 0.7880 - val_loss: 0.3983 - val_auc: 0.7616\n","Epoch 7/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3907 - auc: 0.7965 - val_loss: 0.4006 - val_auc: 0.7602\n","Epoch 8/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3873 - auc: 0.8015Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3873 - auc: 0.8014 - val_loss: 0.4046 - val_auc: 0.7579\n","Epoch 00008: early stopping\n","0.7893695140893404\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4699 - auc: 0.6989 - val_loss: 0.4058 - val_auc: 0.7608\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4083 - auc: 0.7700 - val_loss: 0.3981 - val_auc: 0.7641\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4036 - auc: 0.7779 - val_loss: 0.3960 - val_auc: 0.7661\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4009 - auc: 0.7820 - val_loss: 0.3979 - val_auc: 0.7637\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3984 - auc: 0.7854 - val_loss: 0.3988 - val_auc: 0.7631\n","Epoch 6/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3966 - auc: 0.7883\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3966 - auc: 0.7883 - val_loss: 0.3994 - val_auc: 0.7634\n","Epoch 7/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3902 - auc: 0.7974 - val_loss: 0.4026 - val_auc: 0.7596\n","Epoch 8/50\n","585728/588000 [============================>.] - ETA: 0s - loss: 0.3865 - auc: 0.8025Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3865 - auc: 0.8024 - val_loss: 0.4062 - val_auc: 0.7555\n","Epoch 00008: early stopping\n","0.7878198610118807\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4705 - auc: 0.6985 - val_loss: 0.4052 - val_auc: 0.7591\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4089 - auc: 0.7690 - val_loss: 0.3965 - val_auc: 0.7645\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4034 - auc: 0.7781 - val_loss: 0.3959 - val_auc: 0.7656\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4011 - auc: 0.7815 - val_loss: 0.3957 - val_auc: 0.7656\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3988 - auc: 0.7847 - val_loss: 0.3962 - val_auc: 0.7664\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3970 - auc: 0.7880 - val_loss: 0.3970 - val_auc: 0.7643\n","Epoch 7/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3938 - auc: 0.7924 - val_loss: 0.3993 - val_auc: 0.7607\n","Epoch 8/50\n","585728/588000 [============================>.] - ETA: 0s - loss: 0.3904 - auc: 0.7975Restoring model weights from the end of the best epoch.\n","\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3904 - auc: 0.7975 - val_loss: 0.4016 - val_auc: 0.7579\n","Epoch 00008: early stopping\n","0.7898546679299183\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4684 - auc: 0.6985 - val_loss: 0.4070 - val_auc: 0.7600\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4080 - auc: 0.7708 - val_loss: 0.3970 - val_auc: 0.7632\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4028 - auc: 0.7789 - val_loss: 0.3988 - val_auc: 0.7618\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4005 - auc: 0.7825 - val_loss: 0.3995 - val_auc: 0.7600\n","Epoch 5/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3986 - auc: 0.7852\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3987 - auc: 0.7852 - val_loss: 0.3983 - val_auc: 0.7621\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3936 - auc: 0.7922 - val_loss: 0.4009 - val_auc: 0.7594\n","Epoch 7/50\n","585728/588000 [============================>.] - ETA: 0s - loss: 0.3913 - auc: 0.7957Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3913 - auc: 0.7957 - val_loss: 0.4015 - val_auc: 0.7580\n","Epoch 00007: early stopping\n","0.786652657146754\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 28us/sample - loss: 0.4676 - auc: 0.7016 - val_loss: 0.4065 - val_auc: 0.7568\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4081 - auc: 0.7706 - val_loss: 0.4004 - val_auc: 0.7587\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4032 - auc: 0.7785 - val_loss: 0.4020 - val_auc: 0.7566\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4006 - auc: 0.7822 - val_loss: 0.4051 - val_auc: 0.7568\n","Epoch 5/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3985 - auc: 0.7854\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3985 - auc: 0.7855 - val_loss: 0.4043 - val_auc: 0.7564\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3930 - auc: 0.7930 - val_loss: 0.4041 - val_auc: 0.7542\n","Epoch 7/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3909 - auc: 0.7962Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3909 - auc: 0.7962 - val_loss: 0.4056 - val_auc: 0.7525\n","Epoch 00007: early stopping\n","0.7809701470054708\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4698 - auc: 0.6988 - val_loss: 0.4138 - val_auc: 0.7507\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4084 - auc: 0.7701 - val_loss: 0.4030 - val_auc: 0.7537\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4031 - auc: 0.7785 - val_loss: 0.4014 - val_auc: 0.7562\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4008 - auc: 0.7823 - val_loss: 0.4027 - val_auc: 0.7539\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3983 - auc: 0.7858 - val_loss: 0.4044 - val_auc: 0.7524\n","Epoch 6/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3967 - auc: 0.7880\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3967 - auc: 0.7881 - val_loss: 0.4051 - val_auc: 0.7529\n","Epoch 7/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3901 - auc: 0.7972 - val_loss: 0.4074 - val_auc: 0.7497\n","Epoch 8/50\n","585728/588000 [============================>.] - ETA: 0s - loss: 0.3867 - auc: 0.8025Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3868 - auc: 0.8024 - val_loss: 0.4107 - val_auc: 0.7470\n","Epoch 00008: early stopping\n","0.7797218064840307\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4696 - auc: 0.6990 - val_loss: 0.4081 - val_auc: 0.7561\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4084 - auc: 0.7699 - val_loss: 0.3990 - val_auc: 0.7596\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4031 - auc: 0.7785 - val_loss: 0.3978 - val_auc: 0.7601\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4007 - auc: 0.7820 - val_loss: 0.3976 - val_auc: 0.7575\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3984 - auc: 0.7856 - val_loss: 0.3996 - val_auc: 0.7586\n","Epoch 6/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3963 - auc: 0.7886\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3963 - auc: 0.7885 - val_loss: 0.4014 - val_auc: 0.7558\n","Epoch 7/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3901 - auc: 0.7976Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3901 - auc: 0.7977 - val_loss: 0.4025 - val_auc: 0.7538\n","Epoch 00007: early stopping\n","0.7870516760391106\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4709 - auc: 0.6973 - val_loss: 0.4113 - val_auc: 0.7397\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4084 - auc: 0.7698 - val_loss: 0.3951 - val_auc: 0.7428\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4034 - auc: 0.7781 - val_loss: 0.3945 - val_auc: 0.7421\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4006 - auc: 0.7824 - val_loss: 0.3981 - val_auc: 0.7415\n","Epoch 5/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3989 - auc: 0.7850\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3990 - auc: 0.7849 - val_loss: 0.3994 - val_auc: 0.7394\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3938 - auc: 0.7920 - val_loss: 0.4028 - val_auc: 0.7378\n","Epoch 7/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3908 - auc: 0.7964Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3908 - auc: 0.7964 - val_loss: 0.4041 - val_auc: 0.7365\n","Epoch 00007: early stopping\n","0.7917670737543389\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4713 - auc: 0.6990 - val_loss: 0.4094 - val_auc: 0.7646\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4085 - auc: 0.7698 - val_loss: 0.3959 - val_auc: 0.7688\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4033 - auc: 0.7782 - val_loss: 0.3976 - val_auc: 0.7682\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4007 - auc: 0.7826 - val_loss: 0.3978 - val_auc: 0.7685\n","Epoch 5/50\n","585728/588000 [============================>.] - ETA: 0s - loss: 0.3985 - auc: 0.7854\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3985 - auc: 0.7853 - val_loss: 0.3983 - val_auc: 0.7678\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3933 - auc: 0.7929 - val_loss: 0.4020 - val_auc: 0.7656\n","Epoch 7/50\n","585728/588000 [============================>.] - ETA: 0s - loss: 0.3909 - auc: 0.7963Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3909 - auc: 0.7962 - val_loss: 0.4030 - val_auc: 0.7668\n","Epoch 00007: early stopping\n","0.7878223715694599\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4726 - auc: 0.6971 - val_loss: 0.4038 - val_auc: 0.7409\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4088 - auc: 0.7693 - val_loss: 0.3957 - val_auc: 0.7440\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4032 - auc: 0.7781 - val_loss: 0.3946 - val_auc: 0.7449\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4010 - auc: 0.7816 - val_loss: 0.3958 - val_auc: 0.7426\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3987 - auc: 0.7853 - val_loss: 0.3963 - val_auc: 0.7421\n","Epoch 6/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3969 - auc: 0.7878\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3969 - auc: 0.7877 - val_loss: 0.3963 - val_auc: 0.7406\n","Epoch 7/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3913 - auc: 0.7956Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3913 - auc: 0.7955 - val_loss: 0.3986 - val_auc: 0.7381\n","Epoch 00007: early stopping\n","0.7914337401775576\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 28us/sample - loss: 0.4723 - auc: 0.6975 - val_loss: 0.4073 - val_auc: 0.7317\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4085 - auc: 0.7696 - val_loss: 0.3982 - val_auc: 0.7372\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4031 - auc: 0.7783 - val_loss: 0.3970 - val_auc: 0.7386\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4008 - auc: 0.7823 - val_loss: 0.3964 - val_auc: 0.7391\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3991 - auc: 0.7844 - val_loss: 0.3987 - val_auc: 0.7373\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3968 - auc: 0.7880 - val_loss: 0.3995 - val_auc: 0.7381\n","Epoch 7/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3943 - auc: 0.7916\n","Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3943 - auc: 0.7916 - val_loss: 0.4017 - val_auc: 0.7348\n","Epoch 8/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3863 - auc: 0.8028Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3864 - auc: 0.8028 - val_loss: 0.4044 - val_auc: 0.7317\n","Epoch 00008: early stopping\n","0.7870138579126656\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 28us/sample - loss: 0.4678 - auc: 0.7006 - val_loss: 0.4098 - val_auc: 0.7308\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4084 - auc: 0.7699 - val_loss: 0.3980 - val_auc: 0.7339\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4036 - auc: 0.7780 - val_loss: 0.3978 - val_auc: 0.7334\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4006 - auc: 0.7822 - val_loss: 0.3996 - val_auc: 0.7330\n","Epoch 5/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3989 - auc: 0.7849\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3990 - auc: 0.7848 - val_loss: 0.4014 - val_auc: 0.7325\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3937 - auc: 0.7918 - val_loss: 0.4017 - val_auc: 0.7318\n","Epoch 7/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3910 - auc: 0.7960Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3910 - auc: 0.7960 - val_loss: 0.4057 - val_auc: 0.7283\n","Epoch 00007: early stopping\n","0.7858180565143856\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4678 - auc: 0.7003 - val_loss: 0.4041 - val_auc: 0.7385\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4080 - auc: 0.7704 - val_loss: 0.3950 - val_auc: 0.7430\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4034 - auc: 0.7781 - val_loss: 0.3940 - val_auc: 0.7447\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4010 - auc: 0.7817 - val_loss: 0.3932 - val_auc: 0.7440\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3989 - auc: 0.7850 - val_loss: 0.3937 - val_auc: 0.7442\n","Epoch 6/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3964 - auc: 0.7886\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3964 - auc: 0.7887 - val_loss: 0.3955 - val_auc: 0.7421\n","Epoch 7/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3903 - auc: 0.7973 - val_loss: 0.3986 - val_auc: 0.7401\n","Epoch 8/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3865 - auc: 0.8026Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3865 - auc: 0.8027 - val_loss: 0.4002 - val_auc: 0.7370\n","Epoch 00008: early stopping\n","0.7931556075017328\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 15s 26us/sample - loss: 0.4696 - auc: 0.6987 - val_loss: 0.4094 - val_auc: 0.7486\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4080 - auc: 0.7705 - val_loss: 0.4015 - val_auc: 0.7524\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4029 - auc: 0.7785 - val_loss: 0.4014 - val_auc: 0.7526\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4007 - auc: 0.7822 - val_loss: 0.4021 - val_auc: 0.7528\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3983 - auc: 0.7855 - val_loss: 0.4041 - val_auc: 0.7507\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3964 - auc: 0.7881 - val_loss: 0.4026 - val_auc: 0.7501\n","Epoch 7/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3938 - auc: 0.7924Restoring model weights from the end of the best epoch.\n","\n","Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3937 - auc: 0.7924 - val_loss: 0.4055 - val_auc: 0.7471\n","Epoch 00007: early stopping\n","0.781401484490685\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4688 - auc: 0.7013 - val_loss: 0.4120 - val_auc: 0.7325\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4082 - auc: 0.7701 - val_loss: 0.4014 - val_auc: 0.7365\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4030 - auc: 0.7786 - val_loss: 0.3998 - val_auc: 0.7373\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4005 - auc: 0.7828 - val_loss: 0.4009 - val_auc: 0.7358\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3981 - auc: 0.7860 - val_loss: 0.4006 - val_auc: 0.7378\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3964 - auc: 0.7886 - val_loss: 0.4066 - val_auc: 0.7352\n","Epoch 7/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3935 - auc: 0.7927 - val_loss: 0.4038 - val_auc: 0.7338\n","Epoch 8/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3896 - auc: 0.7985\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3896 - auc: 0.7985 - val_loss: 0.4096 - val_auc: 0.7294\n","Epoch 9/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3803 - auc: 0.8112 - val_loss: 0.4128 - val_auc: 0.7256\n","Epoch 10/50\n","585728/588000 [============================>.] - ETA: 0s - loss: 0.3751 - auc: 0.8183Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3752 - auc: 0.8183 - val_loss: 0.4182 - val_auc: 0.7209\n","Epoch 00010: early stopping\n","0.7824768899060921\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4705 - auc: 0.6978 - val_loss: 0.4075 - val_auc: 0.7664\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4087 - auc: 0.7693 - val_loss: 0.3990 - val_auc: 0.7697\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4031 - auc: 0.7782 - val_loss: 0.3991 - val_auc: 0.7699\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4008 - auc: 0.7820 - val_loss: 0.3998 - val_auc: 0.7710\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3986 - auc: 0.7855 - val_loss: 0.4013 - val_auc: 0.7702\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3968 - auc: 0.7882 - val_loss: 0.4011 - val_auc: 0.7680\n","Epoch 7/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3940 - auc: 0.7920\n","Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3940 - auc: 0.7920 - val_loss: 0.4023 - val_auc: 0.7670\n","Epoch 8/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3865 - auc: 0.8026 - val_loss: 0.4096 - val_auc: 0.7606\n","Epoch 9/50\n","585728/588000 [============================>.] - ETA: 0s - loss: 0.3824 - auc: 0.8085Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3824 - auc: 0.8085 - val_loss: 0.4106 - val_auc: 0.7591\n","Epoch 00009: early stopping\n","0.7863789677121018\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 28us/sample - loss: 0.4691 - auc: 0.7002 - val_loss: 0.4106 - val_auc: 0.7603\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4086 - auc: 0.7695 - val_loss: 0.3975 - val_auc: 0.7630\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4033 - auc: 0.7783 - val_loss: 0.3966 - val_auc: 0.7637\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4009 - auc: 0.7818 - val_loss: 0.4008 - val_auc: 0.7615\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3989 - auc: 0.7850 - val_loss: 0.4000 - val_auc: 0.7603\n","Epoch 6/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3966 - auc: 0.7882\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3966 - auc: 0.7882 - val_loss: 0.4014 - val_auc: 0.7597\n","Epoch 7/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3902 - auc: 0.7973Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3902 - auc: 0.7973 - val_loss: 0.4042 - val_auc: 0.7559\n","Epoch 00007: early stopping\n","0.7871469351732795\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4695 - auc: 0.7001 - val_loss: 0.4119 - val_auc: 0.7488\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4079 - auc: 0.7706 - val_loss: 0.4028 - val_auc: 0.7543\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4030 - auc: 0.7788 - val_loss: 0.4012 - val_auc: 0.7552\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4008 - auc: 0.7822 - val_loss: 0.4029 - val_auc: 0.7543\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3989 - auc: 0.7846 - val_loss: 0.4030 - val_auc: 0.7542\n","Epoch 6/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3966 - auc: 0.7882\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3966 - auc: 0.7882 - val_loss: 0.4026 - val_auc: 0.7540\n","Epoch 7/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3904 - auc: 0.7970Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3904 - auc: 0.7969 - val_loss: 0.4033 - val_auc: 0.7532\n","Epoch 00007: early stopping\n","0.7789959849858027\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4697 - auc: 0.6991 - val_loss: 0.4158 - val_auc: 0.7548\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4083 - auc: 0.7699 - val_loss: 0.4003 - val_auc: 0.7589\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4028 - auc: 0.7790 - val_loss: 0.4003 - val_auc: 0.7611\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4005 - auc: 0.7826 - val_loss: 0.4016 - val_auc: 0.7601\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3985 - auc: 0.7856 - val_loss: 0.4019 - val_auc: 0.7594\n","Epoch 6/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3961 - auc: 0.7891\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3961 - auc: 0.7890 - val_loss: 0.4060 - val_auc: 0.7571\n","Epoch 7/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3892 - auc: 0.7987 - val_loss: 0.4045 - val_auc: 0.7550\n","Epoch 8/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3853 - auc: 0.8043Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3853 - auc: 0.8043 - val_loss: 0.4080 - val_auc: 0.7519\n","Epoch 00008: early stopping\n","0.7859612627721362\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 26us/sample - loss: 0.4700 - auc: 0.6981 - val_loss: 0.4105 - val_auc: 0.7865\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4087 - auc: 0.7696 - val_loss: 0.3963 - val_auc: 0.7917\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4033 - auc: 0.7782 - val_loss: 0.3943 - val_auc: 0.7940\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4007 - auc: 0.7821 - val_loss: 0.3965 - val_auc: 0.7926\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3986 - auc: 0.7855 - val_loss: 0.3961 - val_auc: 0.7912\n","Epoch 6/50\n","585728/588000 [============================>.] - ETA: 0s - loss: 0.3965 - auc: 0.7883\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3965 - auc: 0.7884 - val_loss: 0.3994 - val_auc: 0.7896\n","Epoch 7/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3902 - auc: 0.7973 - val_loss: 0.4010 - val_auc: 0.7864\n","Epoch 8/50\n","585728/588000 [============================>.] - ETA: 0s - loss: 0.3870 - auc: 0.8021Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3870 - auc: 0.8021 - val_loss: 0.4047 - val_auc: 0.7838\n","Epoch 00008: early stopping\n","0.7907912670372532\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4683 - auc: 0.6999 - val_loss: 0.4067 - val_auc: 0.7846\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4079 - auc: 0.7710 - val_loss: 0.3957 - val_auc: 0.7905\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4034 - auc: 0.7780 - val_loss: 0.3949 - val_auc: 0.7911\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4008 - auc: 0.7820 - val_loss: 0.3973 - val_auc: 0.7898\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3984 - auc: 0.7854 - val_loss: 0.3971 - val_auc: 0.7884\n","Epoch 6/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3964 - auc: 0.7883\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3964 - auc: 0.7884 - val_loss: 0.3989 - val_auc: 0.7848\n","Epoch 7/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3896 - auc: 0.7980Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3896 - auc: 0.7980 - val_loss: 0.4012 - val_auc: 0.7814\n","Epoch 00007: early stopping\n","0.7916478724540656\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4680 - auc: 0.7002 - val_loss: 0.4079 - val_auc: 0.7820\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4080 - auc: 0.7705 - val_loss: 0.4000 - val_auc: 0.7894\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4032 - auc: 0.7785 - val_loss: 0.4009 - val_auc: 0.7873\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4005 - auc: 0.7824 - val_loss: 0.4012 - val_auc: 0.7874\n","Epoch 5/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3985 - auc: 0.7851\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3985 - auc: 0.7851 - val_loss: 0.4045 - val_auc: 0.7836\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3938 - auc: 0.7919 - val_loss: 0.4032 - val_auc: 0.7840\n","Epoch 7/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3912 - auc: 0.7956Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3912 - auc: 0.7956 - val_loss: 0.4046 - val_auc: 0.7818\n","Epoch 00007: early stopping\n","0.784312345827566\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 28us/sample - loss: 0.4710 - auc: 0.6982 - val_loss: 0.4073 - val_auc: 0.7877\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4085 - auc: 0.7700 - val_loss: 0.3918 - val_auc: 0.7945\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4034 - auc: 0.7785 - val_loss: 0.3915 - val_auc: 0.7936\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4006 - auc: 0.7826 - val_loss: 0.3933 - val_auc: 0.7919\n","Epoch 5/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3989 - auc: 0.7849\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3989 - auc: 0.7848 - val_loss: 0.3935 - val_auc: 0.7940\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3935 - auc: 0.7925 - val_loss: 0.3942 - val_auc: 0.7911\n","Epoch 7/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3912 - auc: 0.7959Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3912 - auc: 0.7959 - val_loss: 0.3971 - val_auc: 0.7881\n","Epoch 00007: early stopping\n","0.7958790856907038\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 28us/sample - loss: 0.4725 - auc: 0.6943 - val_loss: 0.4061 - val_auc: 0.7704\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4088 - auc: 0.7694 - val_loss: 0.4008 - val_auc: 0.7740\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4033 - auc: 0.7782 - val_loss: 0.4009 - val_auc: 0.7755\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4006 - auc: 0.7825 - val_loss: 0.4007 - val_auc: 0.7754\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3988 - auc: 0.7850 - val_loss: 0.4019 - val_auc: 0.7767\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3967 - auc: 0.7881 - val_loss: 0.4038 - val_auc: 0.7719\n","Epoch 7/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3944 - auc: 0.7915 - val_loss: 0.4059 - val_auc: 0.7705\n","Epoch 8/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3905 - auc: 0.7972\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3905 - auc: 0.7973 - val_loss: 0.4055 - val_auc: 0.7679\n","Epoch 9/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3814 - auc: 0.8099 - val_loss: 0.4123 - val_auc: 0.7640\n","Epoch 10/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3761 - auc: 0.8171Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3761 - auc: 0.8172 - val_loss: 0.4166 - val_auc: 0.7569\n","Epoch 00010: early stopping\n","0.7799939548229794\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 28us/sample - loss: 0.4685 - auc: 0.7002 - val_loss: 0.4052 - val_auc: 0.7567\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4086 - auc: 0.7697 - val_loss: 0.3996 - val_auc: 0.7595\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4036 - auc: 0.7780 - val_loss: 0.3992 - val_auc: 0.7600\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4008 - auc: 0.7824 - val_loss: 0.3996 - val_auc: 0.7587\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3986 - auc: 0.7852 - val_loss: 0.4005 - val_auc: 0.7597\n","Epoch 6/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3964 - auc: 0.7886\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3964 - auc: 0.7885 - val_loss: 0.4030 - val_auc: 0.7555\n","Epoch 7/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3899 - auc: 0.7978Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3899 - auc: 0.7978 - val_loss: 0.4054 - val_auc: 0.7519\n","Epoch 00007: early stopping\n","0.7834983368234101\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 28us/sample - loss: 0.4703 - auc: 0.6984 - val_loss: 0.4104 - val_auc: 0.7551\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4088 - auc: 0.7692 - val_loss: 0.3999 - val_auc: 0.7585\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4035 - auc: 0.7780 - val_loss: 0.3999 - val_auc: 0.7585\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4008 - auc: 0.7822 - val_loss: 0.4010 - val_auc: 0.7595\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3987 - auc: 0.7851 - val_loss: 0.4011 - val_auc: 0.7584\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3964 - auc: 0.7883 - val_loss: 0.4065 - val_auc: 0.7589\n","Epoch 7/50\n","585728/588000 [============================>.] - ETA: 0s - loss: 0.3934 - auc: 0.7928\n","Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3935 - auc: 0.7928 - val_loss: 0.4059 - val_auc: 0.7530\n","Epoch 8/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3854 - auc: 0.8042 - val_loss: 0.4110 - val_auc: 0.7492\n","Epoch 9/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3803 - auc: 0.8113Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3803 - auc: 0.8113 - val_loss: 0.4099 - val_auc: 0.7468\n","Epoch 00009: early stopping\n","0.7846934548136477\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4678 - auc: 0.7009 - val_loss: 0.4135 - val_auc: 0.7530\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4086 - auc: 0.7698 - val_loss: 0.3984 - val_auc: 0.7597\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4036 - auc: 0.7778 - val_loss: 0.3978 - val_auc: 0.7607\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4006 - auc: 0.7825 - val_loss: 0.3983 - val_auc: 0.7605\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3983 - auc: 0.7858 - val_loss: 0.4014 - val_auc: 0.7597\n","Epoch 6/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3964 - auc: 0.7884\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3964 - auc: 0.7884 - val_loss: 0.4008 - val_auc: 0.7576\n","Epoch 7/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3902 - auc: 0.7974Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3902 - auc: 0.7974 - val_loss: 0.4029 - val_auc: 0.7557\n","Epoch 00007: early stopping\n","0.7846419603822788\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4682 - auc: 0.7011 - val_loss: 0.4088 - val_auc: 0.7544\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4081 - auc: 0.7705 - val_loss: 0.4004 - val_auc: 0.7586\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4032 - auc: 0.7787 - val_loss: 0.3997 - val_auc: 0.7598\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4006 - auc: 0.7823 - val_loss: 0.3990 - val_auc: 0.7594\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3989 - auc: 0.7850 - val_loss: 0.4021 - val_auc: 0.7599\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3964 - auc: 0.7888 - val_loss: 0.4027 - val_auc: 0.7571\n","Epoch 7/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3937 - auc: 0.7925 - val_loss: 0.4040 - val_auc: 0.7559\n","Epoch 8/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3901 - auc: 0.7979Restoring model weights from the end of the best epoch.\n","\n","Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3900 - auc: 0.7979 - val_loss: 0.4051 - val_auc: 0.7528\n","Epoch 00008: early stopping\n","0.7832096531547743\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4705 - auc: 0.6996 - val_loss: 0.4120 - val_auc: 0.7828\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4090 - auc: 0.7688 - val_loss: 0.3965 - val_auc: 0.7878\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4033 - auc: 0.7780 - val_loss: 0.3970 - val_auc: 0.7880\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4007 - auc: 0.7824 - val_loss: 0.3969 - val_auc: 0.7870\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3987 - auc: 0.7849 - val_loss: 0.3996 - val_auc: 0.7863\n","Epoch 6/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3965 - auc: 0.7884\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3965 - auc: 0.7886 - val_loss: 0.3987 - val_auc: 0.7849\n","Epoch 7/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3903 - auc: 0.7969Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3903 - auc: 0.7969 - val_loss: 0.4008 - val_auc: 0.7828\n","Epoch 00007: early stopping\n","0.7875249868913933\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4695 - auc: 0.6991 - val_loss: 0.4053 - val_auc: 0.7892\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4082 - auc: 0.7701 - val_loss: 0.3945 - val_auc: 0.7952\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4034 - auc: 0.7780 - val_loss: 0.3937 - val_auc: 0.7947\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4013 - auc: 0.7813 - val_loss: 0.3958 - val_auc: 0.7947\n","Epoch 5/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3985 - auc: 0.7852\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3985 - auc: 0.7853 - val_loss: 0.3954 - val_auc: 0.7936\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3940 - auc: 0.7918 - val_loss: 0.3950 - val_auc: 0.7931\n","Epoch 7/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3917 - auc: 0.7949Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3917 - auc: 0.7950 - val_loss: 0.3959 - val_auc: 0.7928\n","Epoch 00007: early stopping\n","0.793794234275524\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4700 - auc: 0.6983 - val_loss: 0.4077 - val_auc: 0.7818\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4085 - auc: 0.7695 - val_loss: 0.3981 - val_auc: 0.7895\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4032 - auc: 0.7784 - val_loss: 0.3958 - val_auc: 0.7910\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4005 - auc: 0.7824 - val_loss: 0.3961 - val_auc: 0.7894\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3988 - auc: 0.7850 - val_loss: 0.4017 - val_auc: 0.7874\n","Epoch 6/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3962 - auc: 0.7887\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3962 - auc: 0.7887 - val_loss: 0.3979 - val_auc: 0.7862\n","Epoch 7/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3900 - auc: 0.7977 - val_loss: 0.4037 - val_auc: 0.7806\n","Epoch 8/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3863 - auc: 0.8028Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3863 - auc: 0.8028 - val_loss: 0.4076 - val_auc: 0.7757\n","Epoch 00008: early stopping\n","0.7902382665819941\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 27us/sample - loss: 0.4700 - auc: 0.6992 - val_loss: 0.4131 - val_auc: 0.7776\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4079 - auc: 0.7708 - val_loss: 0.4012 - val_auc: 0.7818\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4030 - auc: 0.7785 - val_loss: 0.4009 - val_auc: 0.7814\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4005 - auc: 0.7826 - val_loss: 0.4014 - val_auc: 0.7814\n","Epoch 5/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3988 - auc: 0.7850\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3989 - auc: 0.7849 - val_loss: 0.4039 - val_auc: 0.7794\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3937 - auc: 0.7922 - val_loss: 0.4064 - val_auc: 0.7779\n","Epoch 7/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3917 - auc: 0.7950Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3917 - auc: 0.7950 - val_loss: 0.4057 - val_auc: 0.7766\n","Epoch 00007: early stopping\n","0.7809000469130924\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 16s 28us/sample - loss: 0.4676 - auc: 0.7027 - val_loss: 0.4027 - val_auc: 0.7874\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4081 - auc: 0.7704 - val_loss: 0.3958 - val_auc: 0.7886\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4030 - auc: 0.7787 - val_loss: 0.3956 - val_auc: 0.7898\n","Epoch 4/50\n","588000/588000 [==============================] - 13s 22us/sample - loss: 0.4006 - auc: 0.7823 - val_loss: 0.3966 - val_auc: 0.7899\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3988 - auc: 0.7854 - val_loss: 0.3977 - val_auc: 0.7879\n","Epoch 6/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3967 - auc: 0.7880\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 13s 22us/sample - loss: 0.3967 - auc: 0.7879 - val_loss: 0.3979 - val_auc: 0.7871\n","Epoch 7/50\n","588000/588000 [==============================] - 13s 22us/sample - loss: 0.3907 - auc: 0.7965 - val_loss: 0.4010 - val_auc: 0.7823\n","Epoch 8/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3864 - auc: 0.8028Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 13s 21us/sample - loss: 0.3863 - auc: 0.8028 - val_loss: 0.4057 - val_auc: 0.7780\n","Epoch 00008: early stopping\n","0.7885365775418297\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 17s 28us/sample - loss: 0.4690 - auc: 0.7002 - val_loss: 0.4129 - val_auc: 0.7780\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4080 - auc: 0.7705 - val_loss: 0.4014 - val_auc: 0.7819\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4030 - auc: 0.7788 - val_loss: 0.4010 - val_auc: 0.7829\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.4006 - auc: 0.7822 - val_loss: 0.4005 - val_auc: 0.7818\n","Epoch 5/50\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3984 - auc: 0.7853 - val_loss: 0.4047 - val_auc: 0.7789\n","Epoch 6/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3968 - auc: 0.7879\n","Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3967 - auc: 0.7879 - val_loss: 0.4041 - val_auc: 0.7775\n","Epoch 7/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3898 - auc: 0.7978 - val_loss: 0.4093 - val_auc: 0.7744\n","Epoch 8/50\n","586752/588000 [============================>.] - ETA: 0s - loss: 0.3865 - auc: 0.8025Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 20us/sample - loss: 0.3865 - auc: 0.8025 - val_loss: 0.4114 - val_auc: 0.7711\n","Epoch 00008: early stopping\n","0.7826040174965165\n","Train on 588000 samples, validate on 12000 samples\n","Epoch 1/50\n","588000/588000 [==============================] - 17s 29us/sample - loss: 0.4688 - auc: 0.7003 - val_loss: 0.4066 - val_auc: 0.7806\n","Epoch 2/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4082 - auc: 0.7702 - val_loss: 0.3962 - val_auc: 0.7883\n","Epoch 3/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4028 - auc: 0.7789 - val_loss: 0.3960 - val_auc: 0.7875\n","Epoch 4/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.4009 - auc: 0.7819 - val_loss: 0.3988 - val_auc: 0.7838\n","Epoch 5/50\n","587776/588000 [============================>.] - ETA: 0s - loss: 0.3984 - auc: 0.7854\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3984 - auc: 0.7854 - val_loss: 0.3982 - val_auc: 0.7855\n","Epoch 6/50\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3937 - auc: 0.7921 - val_loss: 0.3987 - val_auc: 0.7849\n","Epoch 7/50\n","585728/588000 [============================>.] - ETA: 0s - loss: 0.3909 - auc: 0.7960Restoring model weights from the end of the best epoch.\n","588000/588000 [==============================] - 12s 21us/sample - loss: 0.3909 - auc: 0.7960 - val_loss: 0.4001 - val_auc: 0.7828\n","Epoch 00007: early stopping\n","0.7877677887250787\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6mgRY187CmL2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":195},"outputId":"ed1005c4-94e9-48e4-bc82-1cc7421913d3","executionInfo":{"status":"ok","timestamp":1585464388157,"user_tz":-540,"elapsed":1060,"user":{"displayName":"迅行","photoUrl":"","userId":"02533846765546745964"}}},"source":["sample.head()"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>600000</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>600001</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>600002</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>600003</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>600004</td>\n","      <td>0.5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id  target\n","0  600000     0.5\n","1  600001     0.5\n","2  600002     0.5\n","3  600003     0.5\n","4  600004     0.5"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"9Tru0KBxCnu2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ba75a693-3617-458e-cc05-266041301f62","executionInfo":{"status":"ok","timestamp":1585473708712,"user_tz":-540,"elapsed":969,"user":{"displayName":"迅行","photoUrl":"","userId":"02533846765546745964"}}},"source":["print(\"Overall AUC={}\".format(metrics.roc_auc_score(train.target.values, oof_preds)))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Overall AUC=0.7856949101893955\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vFSGoRKxmN4y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c7e9f328-e047-4594-bd4c-48cf577a29b8","executionInfo":{"status":"ok","timestamp":1585473771220,"user_tz":-540,"elapsed":2226,"user":{"displayName":"迅行","photoUrl":"","userId":"02533846765546745964"}}},"source":["test_preds /= 50\n","test_ids = test.id.values\n","print(\"Saving submission file\")\n","submission = pd.DataFrame.from_dict({\n","    'id': test_ids,\n","    'target': test_preds\n","})\n","submission.to_csv(\"submission_catindatii.csv\", index=False)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Saving submission file\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4bCoMWLAmWuz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}